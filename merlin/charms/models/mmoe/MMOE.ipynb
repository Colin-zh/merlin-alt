{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dacd7149",
   "metadata": {},
   "source": [
    "# Motivation\n",
    "论文原文：[《Modeling Task Relationships in Multi-task Learning with Multi-gate Mixture-of-Experts》](https://dl.acm.org/doi/abs/10.1145/3219819.3220007)\n",
    "\n",
    "参考学习地址：[多目标学习在推荐系统的应用(MMOE/ESMM/PLE)](https://zhuanlan.zhihu.com/p/291406172)\n",
    "\n",
    "多任务模型通过学习不同任务的联系和差异，可提高每个任务的学习效率和质量。多任务学习框架广泛采用shared-bottom结构，共用底部隐层减少过拟合风险。还有一些其他结构，比如两个任务参数不共用，但是通过对不同任务的参数增加L2范数限制。也有一些对每个任务分别学习一套隐层然后学习所有隐层的组合。和shared-bottom结构相比，这些模型对增加了针对任务的特定参数，在任务差异会影响公共参数的情况下对最终效果有提升。缺点就是模型增加了参数量所以需要更大的数据量来训练模型，而且模型更复杂并不利于在真实生产环境中实际部署使用。\n",
    "\n",
    "<img style=\"display: block; margin: 0 auto;\" src=\"../../../assets/images/mmoe.png\" width = \"800\" height = \"300\" alt=\"MMOE\" align=center />\n",
    "MMoE结构基于Shared-Bottom和MoE结构。\n",
    "\n",
    "## Shared-Bottom Muti-task Model\n",
    "图a）中，shared-bottom网络（表示为函数$f$）位于底部，多个任务共用这一层。往上，K个子任务分别对应一个tower network（表示为$h^k$），每个子任务输出$y_k=h^k(f(x))$\n",
    "\n",
    "## Mixture-of-Experts\n",
    "MoE模型形式化$y=\\sum_{i=1}^{n}g(x)_if_i(x)$，其中$g$为组合专家网络的gating network，有$\\sum_{i=1}^{n}g(x)_i=1$，$f_i,i=1,...,n$分别是专家网络\n",
    "\n",
    "## Multi-gate Mixture-of-Experts\n",
    "核心思想将shared-bottom网络中的函数$f$替换为MoE层，如图c）所示\n",
    "$$\n",
    "\\begin{align}\n",
    "y_k = h^k(f^k(x))=h^k(\\sum_{i=1}^{n}g^k(x)_if_i(x))\n",
    "\\end{align}\n",
    "$$\n",
    "其中$g^k(x)=softmax(W_{gk}x)$，$W_{gk}\\in \\R^{n\\times d}$中n为experts数量，d是特征维度。输入就是input feature，输出是所有experts上的权重。相对于所有任务共用一个门控网络（图b），每个任务单独的gating networks通过最终输出权重不同实现对experts的选择性利用。不同任务的gating networks可以学习到不同的组合experts的模式，因此模型考虑到了捕捉到任务的相关性和区别。"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
