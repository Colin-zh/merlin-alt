{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7829df22",
   "metadata": {},
   "source": [
    "# Motivation\n",
    "论文原文：《》\n",
    "\n",
    "参考学习地址：\n",
    "* [速览 DeepFM: 使用 FM 取代 Wide & Deep 中的 LR](https://zhuanlan.zhihu.com/p/57158486)\n",
    "* [推荐系统系列（一）：FM理论与实践](https://zhuanlan.zhihu.com/p/89639306)\n",
    "\n",
    "相对于传统LR（Logistic Regression），通过特征交叉可以提高CTR预估效果，但Sparse特征会带来的的维度灾难（因为通常交叉项会带来$\\frac{n(n-1)}{2}$个参数）。FM的思想通过对二阶交叉矩阵进行低秩分解（低维矩阵内积），类似Embedding将高维稀疏转化为低位稠密的思路，通过一部分隐向量的信息共享，提高性能的trade-off\n",
    "\n",
    " <img style=\"display: block; margin: 0 auto;\" src=\"../../../assets/images/fm.png\" width = \"600\" height = \"300\" alt=\"FM\" align=center />\n",
    "\n",
    "## 隐向量\n",
    "$$\n",
    "\\begin{align}\n",
    "y=w_0+\\Sigma_{i=1}^nw_ix_i+\\Sigma_{i=1}^{n-1}\\Sigma_{j=i+1}^nw_{ij}x_ix_j\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "FM在公式定义中引入了二阶交叉项，但因为大量特征的one-hot表示之后高度稀疏性问题，可能存在部分$x_ix_j$没有可学习样本或过少导致的过拟合，对应的参数$w_ij$无法充分学习，所以引入了辅助向量（隐向量）$V_i=(v_{i1},v_{i2},...,v_{ik})$ 使得$w_{ij}\\approx V_iV_j^T$。好处有：\n",
    "* 二阶参数量由 $\\frac{n(n-1)}{2}\\rightarrow kn$\n",
    "* 通过共享新向量建立参数间关联，即使$w_{ij}$缺少交叉样本也可以通过仅包含$x_i$或$x_j$的样本更新$<V_i, V_j>$，进而更新 $w_{ij}$\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "y=w_0+\\Sigma_{i=1}^nw_ix_i+\\Sigma_{i=1}^{n-1}\\Sigma_{j=i+1}^n<V_i, V_j>x_ix_j\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "### Tips\n",
    "在二阶项的隐向量计算使用到了<font color=\"red\">对称矩阵求和公式的小trick，也即：$$(\\Sigma a_i)^2 = \\Sigma {a_i}^2 + 2 \\Sigma \\Sigma a_i a_j$$</font>另上式中$a_i=V_ix_i$，则有$$\\Sigma \\Sigma <V_i, V_j>x_ix_j=0.5 * [(\\Sigma V_i x_i)^2 - \\Sigma (V_i x_i)^2]$$\n",
    "\n",
    "带回原公式\n",
    "$$\n",
    "\\begin{align}\n",
    "y&=w_0+\\Sigma_{i=1}^nw_ix_i+\\Sigma_{i=1}^{n-1}\\Sigma_{j=i+1}^nw_{ij}x_ix_j\\\\\n",
    "&=w_0+\\Sigma_{i=1}^nw_ix_i+\\frac{1}{2}\\Sigma_{f=1}^{k}\\{(\\Sigma_{i=1}^nv_{if}x_i)^2-\\Sigma_{i=1}^nv_{if}^2x_i^2\\}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "通过改写可以看到无需交叉项$x_ix_j$就可以表示交叉能力，对应计算$y$的复杂度为$O(kn^2)\\rightarrow O(kn)$。\n",
    "\n",
    "**虽然FM可以应用于任意数值类型的数据上，但需要注意对输入特征数值进行预处理，优先选择特征归一化，其次再进行样本归一化**\n",
    "\n",
    "对比一下后续的衍生工作：\n",
    " <img style=\"display: block; margin: 0 auto;\" src=\"../../../assets/images/fm-comparison.png\" width = \"1000\" height = \"300\" alt=\"FM\" align=center />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173617fa",
   "metadata": {},
   "source": [
    "# FM广告点击率预测\n",
    "FM算法全称为因子分解机（Factorization Machine），思想是在线形回归模型上补充特征的二阶交互，适合捕捉大规模稀疏（类别）特征当中的交互作用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53f88bad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/colin/Desktop\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "DIR_PATH = str(Path(os.getcwd()).parent.parent.parent.parent)\n",
    "sys.path.append(DIR_PATH)\n",
    "\n",
    "print(DIR_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b03508",
   "metadata": {},
   "source": [
    "# 一. 准备数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e76954f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/colin/Desktop/merlin/assets/data/criteo-small/\n"
     ]
    }
   ],
   "source": [
    "print(DIR_PATH + \"/merlin/assets/data/criteo-small/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "78962a1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset URL: https://www.kaggle.com/datasets/leonerd/criteo-small\n",
      "License(s): copyright-authors\n",
      "Downloading criteo-small.zip to /Users/colin/Desktop/merlin/assets/data/criteo-small\n",
      " 99%|█████████████████████████████████████▌| 83.0M/83.9M [00:12<00:00, 9.22MB/s]\n",
      "100%|██████████████████████████████████████| 83.9M/83.9M [00:12<00:00, 7.08MB/s]\n"
     ]
    }
   ],
   "source": [
    "#!/bin/bash\n",
    "!kaggle datasets download leonerd/criteo-small -p /Users/colin/Desktop/merlin/assets/data/criteo-small/ --unzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aeaca036",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8bbcc984",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "dfdata = pd.read_csv(\n",
    "    DIR_PATH + \"/merlin/assets/data/criteo-small/train_1m.txt\", sep='\\t', header=None)\n",
    "dfdata.columns = [\"label\"] + [\"I\"+str(x) for x in range(1,14)] + [\n",
    "    \"C\"+str(x) for x in range(14,40)]\n",
    "\n",
    "target_col = 'label'\n",
    "cat_cols = [x for x in dfdata.columns if x.startswith('C')]\n",
    "num_cols = [x for x in dfdata.columns if x.startswith('I')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "71c38803",
   "metadata": {},
   "outputs": [],
   "source": [
    "dftrain_val, dftest_raw = train_test_split(dfdata, test_size=0.2, random_state=42)\n",
    "dftrain_raw, dfval_raw = train_test_split(dftrain_val, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "72d3f64b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(640000, 40)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dftrain_raw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "94bbb58a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cat_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2b7c12ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:02<00:00,  9.61it/s]\n",
      "/opt/anaconda3/envs/howl/lib/python3.12/site-packages/sklearn/pipeline.py:61: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/howl/lib/python3.12/site-packages/sklearn/pipeline.py:61: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Shape of passed values is (640000, 44), indices imply (640000, 39)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m pipe = TabularPreprocessor(cat_features = cat_cols, onehot_max_cat_num=\u001b[32m3\u001b[39m)\n\u001b[32m      6\u001b[39m encoder = OrdinalEncoder()\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m dftrain = pipe.fit_transform(dftrain_raw.drop(target_col,axis=\u001b[32m1\u001b[39m))\n\u001b[32m      9\u001b[39m dftrain[target_col] = encoder.fit_transform(\n\u001b[32m     10\u001b[39m     dftrain_raw[target_col].values.reshape(-\u001b[32m1\u001b[39m,\u001b[32m1\u001b[39m)).astype(np.int32)\n\u001b[32m     12\u001b[39m dfval = pipe.transform(dfval_raw.drop(target_col,axis=\u001b[32m1\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/howl/lib/python3.12/site-packages/sklearn/utils/_set_output.py:316\u001b[39m, in \u001b[36m_wrap_method_output.<locals>.wrapped\u001b[39m\u001b[34m(self, X, *args, **kwargs)\u001b[39m\n\u001b[32m    314\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[32m    315\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m316\u001b[39m     data_to_wrap = f(\u001b[38;5;28mself\u001b[39m, X, *args, **kwargs)\n\u001b[32m    317\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    318\u001b[39m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[32m    319\u001b[39m         return_tuple = (\n\u001b[32m    320\u001b[39m             _wrap_data_with_container(method, data_to_wrap[\u001b[32m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[32m    321\u001b[39m             *data_to_wrap[\u001b[32m1\u001b[39m:],\n\u001b[32m    322\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/howl/lib/python3.12/site-packages/sklearn/base.py:894\u001b[39m, in \u001b[36mTransformerMixin.fit_transform\u001b[39m\u001b[34m(self, X, y, **fit_params)\u001b[39m\n\u001b[32m    879\u001b[39m         warnings.warn(\n\u001b[32m    880\u001b[39m             (\n\u001b[32m    881\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThis object (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m) has a `transform`\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    889\u001b[39m             \u001b[38;5;167;01mUserWarning\u001b[39;00m,\n\u001b[32m    890\u001b[39m         )\n\u001b[32m    892\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    893\u001b[39m     \u001b[38;5;66;03m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m894\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.fit(X, **fit_params).transform(X)\n\u001b[32m    895\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    896\u001b[39m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[32m    897\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.fit(X, y, **fit_params).transform(X)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/howl/lib/python3.12/site-packages/sklearn/utils/_set_output.py:316\u001b[39m, in \u001b[36m_wrap_method_output.<locals>.wrapped\u001b[39m\u001b[34m(self, X, *args, **kwargs)\u001b[39m\n\u001b[32m    314\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[32m    315\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m316\u001b[39m     data_to_wrap = f(\u001b[38;5;28mself\u001b[39m, X, *args, **kwargs)\n\u001b[32m    317\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    318\u001b[39m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[32m    319\u001b[39m         return_tuple = (\n\u001b[32m    320\u001b[39m             _wrap_data_with_container(method, data_to_wrap[\u001b[32m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[32m    321\u001b[39m             *data_to_wrap[\u001b[32m1\u001b[39m:],\n\u001b[32m    322\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/merlin/charms/datasets/preprocess.py:228\u001b[39m, in \u001b[36mTabularPreprocessor.transform\u001b[39m\u001b[34m(self, X, y)\u001b[39m\n\u001b[32m    226\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtransform\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: pd.DataFrame, y = \u001b[38;5;28;01mNone\u001b[39;00m) -> pd.DataFrame:\n\u001b[32m    227\u001b[39m     Xout = \u001b[38;5;28mself\u001b[39m.pipeline.transform(X)\n\u001b[32m--> \u001b[39m\u001b[32m228\u001b[39m     dfout = pd.DataFrame(Xout,columns = \u001b[38;5;28mself\u001b[39m.feature_names,index = X.index)\n\u001b[32m    229\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m dfout.columns:\n\u001b[32m    230\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.embedding_features:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/howl/lib/python3.12/site-packages/pandas/core/frame.py:827\u001b[39m, in \u001b[36mDataFrame.__init__\u001b[39m\u001b[34m(self, data, index, columns, dtype, copy)\u001b[39m\n\u001b[32m    816\u001b[39m         mgr = dict_to_mgr(\n\u001b[32m    817\u001b[39m             \u001b[38;5;66;03m# error: Item \"ndarray\" of \"Union[ndarray, Series, Index]\" has no\u001b[39;00m\n\u001b[32m    818\u001b[39m             \u001b[38;5;66;03m# attribute \"name\"\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    824\u001b[39m             copy=_copy,\n\u001b[32m    825\u001b[39m         )\n\u001b[32m    826\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m827\u001b[39m         mgr = ndarray_to_mgr(\n\u001b[32m    828\u001b[39m             data,\n\u001b[32m    829\u001b[39m             index,\n\u001b[32m    830\u001b[39m             columns,\n\u001b[32m    831\u001b[39m             dtype=dtype,\n\u001b[32m    832\u001b[39m             copy=copy,\n\u001b[32m    833\u001b[39m             typ=manager,\n\u001b[32m    834\u001b[39m         )\n\u001b[32m    836\u001b[39m \u001b[38;5;66;03m# For data is list-like, or Iterable (will consume into list)\u001b[39;00m\n\u001b[32m    837\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m is_list_like(data):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/howl/lib/python3.12/site-packages/pandas/core/internals/construction.py:336\u001b[39m, in \u001b[36mndarray_to_mgr\u001b[39m\u001b[34m(values, index, columns, dtype, copy, typ)\u001b[39m\n\u001b[32m    331\u001b[39m \u001b[38;5;66;03m# _prep_ndarraylike ensures that values.ndim == 2 at this point\u001b[39;00m\n\u001b[32m    332\u001b[39m index, columns = _get_axes(\n\u001b[32m    333\u001b[39m     values.shape[\u001b[32m0\u001b[39m], values.shape[\u001b[32m1\u001b[39m], index=index, columns=columns\n\u001b[32m    334\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m336\u001b[39m _check_values_indices_shape_match(values, index, columns)\n\u001b[32m    338\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m typ == \u001b[33m\"\u001b[39m\u001b[33marray\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    339\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(values.dtype.type, \u001b[38;5;28mstr\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/howl/lib/python3.12/site-packages/pandas/core/internals/construction.py:420\u001b[39m, in \u001b[36m_check_values_indices_shape_match\u001b[39m\u001b[34m(values, index, columns)\u001b[39m\n\u001b[32m    418\u001b[39m passed = values.shape\n\u001b[32m    419\u001b[39m implied = (\u001b[38;5;28mlen\u001b[39m(index), \u001b[38;5;28mlen\u001b[39m(columns))\n\u001b[32m--> \u001b[39m\u001b[32m420\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mShape of passed values is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpassed\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, indices imply \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimplied\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mValueError\u001b[39m: Shape of passed values is (640000, 44), indices imply (640000, 39)"
     ]
    }
   ],
   "source": [
    "from merlin.charms.datasets.preprocess import TabularPreprocessor\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "#特征工程\n",
    "pipe = TabularPreprocessor(cat_features=cat_cols, onehot_max_cat_num=3)\n",
    "encoder = OrdinalEncoder()\n",
    "\n",
    "dftrain = pipe.fit_transform(dftrain_raw.drop(target_col, axis=1))\n",
    "dftrain[target_col] = encoder.fit_transform(\n",
    "    dftrain_raw[target_col].values.reshape(-1, 1)).astype(np.int32)\n",
    "\n",
    "dfval = pipe.transform(dfval_raw.drop(target_col, axis=1))\n",
    "dfval[target_col] = encoder.transform(\n",
    "    dfval_raw[target_col].values.reshape(-1, 1)).astype(np.int32)\n",
    "\n",
    "dftest = pipe.transform(dftest_raw.drop(target_col, axis=1))\n",
    "dftest[target_col] = encoder.transform(\n",
    "    dftest_raw[target_col].values.reshape(-1, 1)).astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760e2a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from merlin.charms.datasets.dataset import TabularDataset\n",
    "\n",
    "def get_dataset(dfdata):\n",
    "    return TabularDataset(\n",
    "        data = dfdata,\n",
    "        task = \"binary\",\n",
    "        target = [target_col],\n",
    "        continuous_cols = pipe.get_numeric_features(),\n",
    "        categorical_cols = pipe.get_embedding_features(),\n",
    "    )\n",
    "\n",
    "def get_dataloader(ds, batch_size=512, num_workers=0, shuffle=False):\n",
    "    return DataLoader(\n",
    "        dataset=ds,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=False,\n",
    "    )\n",
    "\n",
    "ds_train = get_dataset(dftrain)\n",
    "ds_val = get_dataset(dfval)\n",
    "ds_test = get_dataset(dftest)\n",
    "\n",
    "dl_train = get_dataloader(ds_train, batch_size=2048, shuffle=True)\n",
    "dl_val = get_dataloader(ds_val, shuffle=False)\n",
    "dl_test = get_dataloader(ds_test, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070b37b2",
   "metadata": {},
   "source": [
    "# 二. 定义模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2e87b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from merlin.charms.models.fm import FMModel, FMConfig\n",
    "\n",
    "model_config = FMConfig(task=\"binary\")\n",
    "config = model_config.merge_dataset_config(ds_train)\n",
    "\n",
    "print('input_embed_dim = ', config.input_embed_dim)\n",
    "print('\\n categorical_cardinality = ',config.categorical_cardinality)\n",
    "print('\\n embedding_dims = ' , config.embedding_dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55dedc88",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = FMModel(config)\n",
    "\n",
    "# 初始化参数\n",
    "net.reset_weights()\n",
    "net.data_aware_initialization(dl_train)\n",
    "\n",
    "print(net.backbone.output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8470c17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in dl_train:\n",
    "    break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a907dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = net.forward(batch)\n",
    "loss = net.compute_loss(output,batch['target'])\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ace930",
   "metadata": {},
   "source": [
    "# 三. 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9955796",
   "metadata": {},
   "outputs": [],
   "source": [
    "from merlin.tools import WandModel\n",
    "from merlin.tools.metrics import AUC\n",
    "\n",
    "optimizer = torch.optim.AdamW(net.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "wand_model = WandModel(\n",
    "    model=net,\n",
    "    optimizer=optimizer,\n",
    "    metrics_dict={\"auc\": AUC()}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a64701d",
   "metadata": {},
   "outputs": [],
   "source": [
    "wand_model.fit(\n",
    "    train_data=dl_train,\n",
    "    val_data=dl_val,\n",
    "    ckpt_path=DIR_PATH + \"/merlin/assets/checkpoints/fm_criteo_binary\",\n",
    "    epochs=30,\n",
    "    patience=5,\n",
    "    monitor=\"val_auc\",\n",
    "    mode=\"max\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ff1882",
   "metadata": {},
   "source": [
    "# 四. 评估模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819def78",
   "metadata": {},
   "outputs": [],
   "source": [
    "wand_model.evaluate(dl_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea725be",
   "metadata": {},
   "outputs": [],
   "source": [
    "wand_model.evaluate(dl_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2835e9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "wand_model.evaluate(dl_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e272c0",
   "metadata": {},
   "source": [
    "# 五. 使用模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22aea5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "net, dl_test = wand_model.accelerator.prepare(net, dl_test)\n",
    "net.eval()\n",
    "preds = []\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(dl_test):\n",
    "        preds.append(net.predict(batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508baa98",
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_list = [yd.sigmoid().reshape(-1).tolist() for yd in preds]\n",
    "yhat = []\n",
    "for yd in yhat_list:\n",
    "    yhat.extend(yd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba957b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "dftest_raw = dftest_raw.rename(columns={target_col: 'y'})\n",
    "dftest_raw['yhat'] = yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e485591",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "roc_auc_score(dftest_raw['y'], dftest_raw['yhat'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a419e569",
   "metadata": {},
   "source": [
    "# 六. 保存模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac1e5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "net.load_state_dict(torch.load('checkpoint', weights_only=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "howl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
